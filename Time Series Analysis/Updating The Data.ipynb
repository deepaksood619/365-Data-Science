{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary package \n",
    "import yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warning messages\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 downloaded\n"
     ]
    }
   ],
   "source": [
    "# Using the .download() method to get our data\n",
    "\n",
    "raw_data = yfinance.download (tickers = \"^GSPC ^FTSE ^N225 ^GDAXI\", start = \"1994-01-07\", end = \"2019-09-27\", interval = \"1d\", group_by = 'ticker', auto_adjust = True, treads = True)\n",
    "\n",
    "# tickers -> The time series we are interested in - (in our case, these are the S&P, FTSE, NIKKEI and DAX)\n",
    "# start -> The starting date of our data set\n",
    "# end -> The ending date of our data set (at the time of upload, this is the current date)\n",
    "# interval -> The distance in time between two recorded observations. Since we're using daily closing prices, we set it equal to \"1d\", which indicates 1 day. \n",
    "# group_by -> The way we want to group the scraped data. Usually we want it to be \"ticker\", so that we have all the information about a time series in 1 variable.\n",
    "# auto_adjust -> Automatically adjust the closing prices for each period. \n",
    "# treads - > Whether to use threads for mass downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a back up copy in case we remove/alter elements of the data by mistake\n",
    "df_comp = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns to the data set\n",
    "df_comp['spx'] = df_comp['^GSPC'].Close\n",
    "df_comp['dax'] = df_comp['^GDAXI'].Close\n",
    "df_comp['ftse'] = df_comp['^FTSE'].Close\n",
    "df_comp['nikkei'] = df_comp['^N225'].Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comp = df_comp.iloc[1:] # Removing the first elements, since we always start 1 period before the first, due to time zone differences of closing prices\n",
    "del df_comp['^N225']  # Removing the original tickers of the data set\n",
    "del df_comp['^GSPC']\n",
    "del df_comp['^GDAXI']\n",
    "del df_comp['^FTSE']\n",
    "df_comp=df_comp.asfreq('b') # Setting the frequency of the data\n",
    "df_comp=df_comp.fillna(method='ffill') # Filling any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               spx      dax    ftse    nikkei\n",
      "                                             \n",
      "Date                                         \n",
      "1994-01-07  469.90  2224.95  3446.0  18124.01\n",
      "1994-01-10  475.27  2225.00  3440.6  18443.44\n",
      "1994-01-11  474.13  2228.10  3413.8  18485.25\n",
      "1994-01-12  474.17  2182.06  3372.0  18793.88\n",
      "1994-01-13  472.47  2142.37  3360.0  18577.26\n",
      "                spx       dax    ftse    nikkei\n",
      "                                               \n",
      "Date                                           \n",
      "2019-09-20  2992.07  12468.01  7344.9  22079.09\n",
      "2019-09-23  2991.78  12342.33  7326.1  22079.09\n",
      "2019-09-24  2966.60  12307.15  7291.4  22098.84\n",
      "2019-09-25  2984.87  12234.18  7290.0  22020.15\n",
      "2019-09-26  2977.62  12288.54  7351.1  22048.24\n"
     ]
    }
   ],
   "source": [
    "print (df_comp.head()) # Displaying the first 5 elements to make sure the data was scraped correctly\n",
    "print (df_comp.tail()) # Making sure the last day we're including in the series are correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
